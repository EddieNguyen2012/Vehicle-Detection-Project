{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
=======
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jTKSpQmfvCl",
        "outputId": "1a18ec18-0f3a-4ee1-f475-5ca71ca1d048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.16.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LcfhkMvO97kA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF9d3Rs-3bOm",
        "outputId": "a806c19c-7022-48cb-c5a2-d799233983d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UsakXbzP5sQB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Define paths to your image folders\n",
        "# non_vehicle_folder = \"vehicle_dataset/non-vehicles/\"\n",
        "# vehicle_folder = \"vehicle_dataset/vehicles/\"\n",
        "\n",
        "# # Step 1: List Image Files using os.listdir()\n",
        "# non_vehicle_files = [os.path.join(non_vehicle_folder, file) for file in os.listdir(non_vehicle_folder) if file.endswith('.png')]\n",
        "# vehicle_files = [os.path.join(vehicle_folder, file) for file in os.listdir(vehicle_folder) if file.endswith('.png')]\n",
        "\n",
        "# # Step 2: Create Dataset with Labels\n",
        "# non_vehicle_dataset = tf.data.Dataset.from_tensor_slices((non_vehicle_files, [1] * len(non_vehicle_files)))\n",
        "# vehicle_dataset = tf.data.Dataset.from_tensor_slices((vehicle_files, [2] * len(vehicle_files)))\n",
        "\n",
        "# # Step 3: Combine Datasets\n",
        "# image_dataset = non_vehicle_dataset.concatenate(vehicle_dataset)\n",
        "\n",
        "images = []\n",
        "labels = []\n",
        "dataset = 'vehicle_dataset'\n",
        "\n",
        "for folder in os.listdir(dataset):\n",
        "    if not folder.startswith('.'):\n",
        "        label = None\n",
        "\n",
        "        if folder in ['vehicles']: label = 0\n",
        "        elif folder in ['non-vehicles']: label = 1\n",
        "\n",
        "        for file in os.listdir(os.path.join(dataset, folder)):\n",
        "            if file.endswith('.png'):\n",
        "                img_path = os.path.join(os.path.join(dataset, folder), file)\n",
        "\n",
        "                image = cv2.imread(img_path)\n",
        "\n",
        "                images.append(image)\n",
        "                labels.append(label)\n",
        "\n",
        "images = np.array(images, dtype='float32')/255.0\n",
        "labels = np.array(labels, dtype='int32')\n",
        "\n",
        "images = images.reshape(-1,64,64,3)\n",
        "\n",
        "images, labels = shuffle(images, labels, random_state=42)\n",
        "\n",
        "xtrain,xval,ytrain,yval=train_test_split(images, labels,train_size=0.8,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(64, 64, 3)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "OOP6IYPW99HB",
        "outputId": "08383d61-9226-4bcf-ffc8-5e26bcc463de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/davidtan/Documents/CS171/FinalProject/Vehicle-Detection-Project/venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# # Define the model architecture\n",
        "# class SSDModel(tf.keras.Model):\n",
        "#     def __init__(self, num_classes):\n",
        "#         super(SSDModel, self).__init__()\n",
        "#         self.num_classes = num_classes\n",
        "\n",
        "#         # Backbone network (e.g. VGG16)\n",
        "#         self.backbone = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
        "\n",
        "#         # Feature extractor layers\n",
        "#         self.feature_extractor = tf.keras.Sequential([\n",
        "#             tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same'),\n",
        "#             tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same'),\n",
        "#             tf.keras.layers.MaxPooling2D((2, 2))\n",
        "#         ])\n",
        "\n",
        "#         # SSD layers\n",
        "#         self.ssd_layers = []\n",
        "#         for i in range(6):  # 6 default boxes per feature map\n",
        "#             self.ssd_layers.append(SSDLayer(4 + i, num_classes))\n",
        "\n",
        "#         # Prediction layers\n",
        "#         self.prediction_layers = []\n",
        "#         for i in range(6):  # 6 feature maps\n",
        "#             self.prediction_layers.append(PredictionLayer(num_classes))\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         # Backbone network\n",
        "#         x = self.backbone(inputs)\n",
        "\n",
        "#         # Feature extractor layers\n",
        "#         x = self.feature_extractor(x)\n",
        "\n",
        "#         # SSD layers\n",
        "#         feature_maps = []\n",
        "#         for i, layer in enumerate(self.ssd_layers):\n",
        "#             print(layer)\n",
        "#             x, feature_map = layer(x)\n",
        "#             feature_maps.append(feature_map)\n",
        "\n",
        "#         # Prediction layers\n",
        "#         predictions = []\n",
        "#         for i, layer in enumerate(self.prediction_layers):\n",
        "#             predictions.append(layer(feature_maps[i]))\n",
        "\n",
        "#         # Concatenate predictions\n",
        "#         predictions = tf.concat(predictions, axis=1)\n",
        "\n",
        "#         return predictions\n",
        "\n",
        "# class SSDLayer(tf.keras.layers.Layer):\n",
        "#     def __init__(self, num_defaults, num_classes):\n",
        "#         super(SSDLayer, self).__init__()\n",
        "#         self.num_defaults = num_defaults\n",
        "#         self.num_classes = num_classes\n",
        "\n",
        "#     def build(self, input_shape):\n",
        "#         self.conv = tf.keras.layers.Conv2D(self.num_defaults * (4 + self.num_classes), (3, 3), activation='linear', padding='same', input_shape=input_shape)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         # inputs = tf.reshape(inputs, (-1, 64, 64, 3))  # Reshape inputs here\n",
        "#         inputs = tf.reshape(inputs, (-1, 32, 32, 3))  # Assuming 3 channels (RGB)\n",
        "#         x = self.conv(inputs)\n",
        "#         x = tf.reshape(x, (-1, x.shape[1], x.shape[2], self.num_defaults, 4 + self.num_classes))\n",
        "#         return x, x[:, :, :, :, :4]\n",
        "\n",
        "# class PredictionLayer(tf.keras.layers.Layer):\n",
        "#     def __init__(self, num_classes):\n",
        "#         super(PredictionLayer, self).__init__()\n",
        "#         self.num_classes = num_classes\n",
        "\n",
        "#     def build(self, input_shape):\n",
        "#         self.conv = tf.keras.layers.Conv2D(self.num_classes, (3, 3), activation='softmax', padding='same', input_shape=input_shape)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         x = self.conv(inputs)\n",
        "#         return x\n",
        "\n",
        "# def load_image(file_path, label):\n",
        "#     img = tf.io.read_file(file_path)\n",
        "#     img = tf.image.decode_image(img, channels=3)  # Ensure 3 channels for color images\n",
        "#     img = tf.cast(img, tf.uint8)  # Cast the image to uint8 data type\n",
        "#     return img, label\n",
        "\n",
        "\n",
        "# # Create the model\n",
        "# model = SSDModel(num_classes=2)  # 2 classes: vehicle and non-vehicle\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='ssd_loss', metrics=['accuracy'])\n",
        "\n",
        "#\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(64,64,3)))\n",
        "model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        " \n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(50, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])\n",
        "\n",
        "# # Create a dataset object\n",
        "# dataset = image_dataset.map(load_image)\n",
        "\n",
        "# # # Define the size of the training set\n",
        "# # train_size = int(0.8 * dataset.cardinality().numpy())\n",
        "\n",
        "\n",
        "# # # Split the dataset into training and testing subsets\n",
        "# # train_dataset = dataset.take(train_size)\n",
        "# # test_dataset = dataset.skip(train_size)\n",
        "\n",
        "# # Convert dataset to list to calculate train_size\n",
        "# dataset_list = list(dataset)\n",
        "# train_size = int(0.8 * len(dataset_list))\n",
        "\n",
        "# # Split the dataset into training and testing subsets\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices(dataset_list[:train_size])\n",
        "# test_dataset = tf.data.Dataset.from_tensor_slices(dataset_list[train_size:])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        featurewise_center=False,  \n",
        "        samplewise_center=False,  \n",
        "        featurewise_std_normalization=False,  \n",
        "        samplewise_std_normalization=False,  \n",
        "        zca_whitening=False,    \n",
        "        rotation_range=15,    \n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,  \n",
        "        horizontal_flip=True,  \n",
        "        vertical_flip=False)\n",
        "datagen.fit(xtrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 215ms/step - accuracy: 0.8663 - loss: 0.3078 - val_accuracy: 0.9724 - val_loss: 0.0739\n",
            "Epoch 2/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 210ms/step - accuracy: 0.9777 - loss: 0.0709 - val_accuracy: 0.9820 - val_loss: 0.0496\n",
            "Epoch 3/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 219ms/step - accuracy: 0.9844 - loss: 0.0477 - val_accuracy: 0.9797 - val_loss: 0.0684\n",
            "Epoch 4/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 215ms/step - accuracy: 0.9851 - loss: 0.0455 - val_accuracy: 0.9870 - val_loss: 0.0353\n",
            "Epoch 5/5\n",
            "\u001b[1m444/444\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 214ms/step - accuracy: 0.9873 - loss: 0.0402 - val_accuracy: 0.9901 - val_loss: 0.0305\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x2e1c3ab90>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(xtrain, ytrain, batch_size=32,\n",
        "                    epochs=5,\n",
        "                    verbose=1,\n",
        "                    validation_data=(xval, yval))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
>>>>>>> e32e985 (Update model)
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jTKSpQmfvCl",
    "outputId": "1a18ec18-0f3a-4ee1-f475-5ca71ca1d048",
    "ExecuteTime": {
     "end_time": "2024-05-05T02:59:39.502588Z",
     "start_time": "2024-05-05T02:59:39.495756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LcfhkMvO97kA",
    "ExecuteTime": {
     "end_time": "2024-05-05T02:59:39.503748Z",
     "start_time": "2024-05-05T02:59:39.499511Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF9d3Rs-3bOm",
    "outputId": "a806c19c-7022-48cb-c5a2-d799233983d7",
    "ExecuteTime": {
     "end_time": "2024-05-05T02:59:39.507118Z",
     "start_time": "2024-05-05T02:59:39.502651Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UsakXbzP5sQB",
    "ExecuteTime": {
     "end_time": "2024-05-05T02:59:39.509295Z",
     "start_time": "2024-05-05T02:59:39.507246Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T02:59:39.561159Z",
     "start_time": "2024-05-05T02:59:39.510455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define paths to your image folders\n",
    "non_vehicle_folder = \"data/non-vehicles/\"\n",
    "vehicle_folder = \"data/vehicles/\"\n",
    "\n",
    "# Step 1: List Image Files using os.listdir()\n",
    "non_vehicle_files = [os.path.join(non_vehicle_folder, file) for file in os.listdir(non_vehicle_folder) if file.endswith('.png')]\n",
    "vehicle_files = [os.path.join(vehicle_folder, file) for file in os.listdir(vehicle_folder) if file.endswith('.png')]\n",
    "\n",
    "# Step 2: Create Dataset with Labels\n",
    "non_vehicle_dataset = tf.data.Dataset.from_tensor_slices((non_vehicle_files, [1] * len(non_vehicle_files)))\n",
    "vehicle_dataset = tf.data.Dataset.from_tensor_slices((vehicle_files, [2] * len(vehicle_files)))\n",
    "\n",
    "# Step 3: Combine Datasets\n",
    "image_dataset = non_vehicle_dataset.concatenate(vehicle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "OOP6IYPW99HB",
    "outputId": "08383d61-9226-4bcf-ffc8-5e26bcc463de",
    "ExecuteTime": {
     "end_time": "2024-05-05T03:02:20.042055Z",
     "start_time": "2024-05-05T03:02:19.517256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mSSLCertVerificationError\u001B[0m                  Traceback (most recent call last)",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1344\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[0;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[1;32m   1343\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1344\u001B[0m     \u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1345\u001B[0m \u001B[43m              \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhas_header\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTransfer-encoding\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1346\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1319\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[0;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1319\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1365\u001B[0m, in \u001B[0;36mHTTPConnection._send_request\u001B[0;34m(self, method, url, body, headers, encode_chunked)\u001B[0m\n\u001B[1;32m   1364\u001B[0m     body \u001B[38;5;241m=\u001B[39m _encode(body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbody\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m-> 1365\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1314\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1313\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[0;32m-> 1314\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1074\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1073\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[0;32m-> 1074\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1076\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1077\u001B[0m \n\u001B[1;32m   1078\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1018\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[0;32m-> 1018\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1019\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:1460\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1458\u001B[0m     server_hostname \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[0;32m-> 1460\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrap_socket\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1461\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_hostname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:455\u001B[0m, in \u001B[0;36mSSLContext.wrap_socket\u001B[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001B[0m\n\u001B[1;32m    449\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrap_socket\u001B[39m(\u001B[38;5;28mself\u001B[39m, sock, server_side\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    450\u001B[0m                 do_handshake_on_connect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    451\u001B[0m                 suppress_ragged_eofs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    452\u001B[0m                 server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, session\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001B[39;00m\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;66;03m# ctx._wrap_socket()\u001B[39;00m\n\u001B[0;32m--> 455\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msslsocket_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    456\u001B[0m \u001B[43m        \u001B[49m\u001B[43msock\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msock\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserver_side\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_side\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdo_handshake_on_connect\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m        \u001B[49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msuppress_ragged_eofs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    461\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    462\u001B[0m \u001B[43m        \u001B[49m\u001B[43msession\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msession\u001B[49m\n\u001B[1;32m    463\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1046\u001B[0m, in \u001B[0;36mSSLSocket._create\u001B[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001B[0m\n\u001B[1;32m   1045\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdo_handshake_on_connect should not be specified for non-blocking sockets\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1046\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1047\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mOSError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1317\u001B[0m, in \u001B[0;36mSSLSocket.do_handshake\u001B[0;34m(self, block)\u001B[0m\n\u001B[1;32m   1316\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msettimeout(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1317\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1318\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "\u001B[0;31mSSLCertVerificationError\u001B[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mURLError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/Vehicle-Detection-Project/.venv/lib/python3.12/site-packages/keras/src/utils/file_utils.py:291\u001B[0m, in \u001B[0;36mget_file\u001B[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001B[0m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 291\u001B[0m     \u001B[43murlretrieve\u001B[49m\u001B[43m(\u001B[49m\u001B[43morigin\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDLProgbar\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m urllib\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mHTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:240\u001B[0m, in \u001B[0;36murlretrieve\u001B[0;34m(url, filename, reporthook, data)\u001B[0m\n\u001B[1;32m    238\u001B[0m url_type, path \u001B[38;5;241m=\u001B[39m _splittype(url)\n\u001B[0;32m--> 240\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mclosing(\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[1;32m    241\u001B[0m     headers \u001B[38;5;241m=\u001B[39m fp\u001B[38;5;241m.\u001B[39minfo()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:215\u001B[0m, in \u001B[0;36murlopen\u001B[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001B[0m\n\u001B[1;32m    214\u001B[0m     opener \u001B[38;5;241m=\u001B[39m _opener\n\u001B[0;32m--> 215\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mopener\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:515\u001B[0m, in \u001B[0;36mOpenerDirector.open\u001B[0;34m(self, fullurl, data, timeout)\u001B[0m\n\u001B[1;32m    514\u001B[0m sys\u001B[38;5;241m.\u001B[39maudit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124murllib.Request\u001B[39m\u001B[38;5;124m'\u001B[39m, req\u001B[38;5;241m.\u001B[39mfull_url, req\u001B[38;5;241m.\u001B[39mdata, req\u001B[38;5;241m.\u001B[39mheaders, req\u001B[38;5;241m.\u001B[39mget_method())\n\u001B[0;32m--> 515\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;66;03m# post-process response\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:532\u001B[0m, in \u001B[0;36mOpenerDirector._open\u001B[0;34m(self, req, data)\u001B[0m\n\u001B[1;32m    531\u001B[0m protocol \u001B[38;5;241m=\u001B[39m req\u001B[38;5;241m.\u001B[39mtype\n\u001B[0;32m--> 532\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_open\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprotocol\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[1;32m    533\u001B[0m \u001B[43m                          \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_open\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    534\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:492\u001B[0m, in \u001B[0;36mOpenerDirector._call_chain\u001B[0;34m(self, chain, kind, meth_name, *args)\u001B[0m\n\u001B[1;32m    491\u001B[0m func \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(handler, meth_name)\n\u001B[0;32m--> 492\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1392\u001B[0m, in \u001B[0;36mHTTPSHandler.https_open\u001B[0;34m(self, req)\u001B[0m\n\u001B[1;32m   1391\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhttps_open\u001B[39m(\u001B[38;5;28mself\u001B[39m, req):\n\u001B[0;32m-> 1392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhttp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mHTTPSConnection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1393\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py:1347\u001B[0m, in \u001B[0;36mAbstractHTTPHandler.do_open\u001B[0;34m(self, http_class, req, **http_conn_args)\u001B[0m\n\u001B[1;32m   1346\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err: \u001B[38;5;66;03m# timeout error\u001B[39;00m\n\u001B[0;32m-> 1347\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m URLError(err)\n\u001B[1;32m   1348\u001B[0m r \u001B[38;5;241m=\u001B[39m h\u001B[38;5;241m.\u001B[39mgetresponse()\n",
      "\u001B[0;31mURLError\u001B[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 84\u001B[0m\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img, label\n\u001B[1;32m     83\u001B[0m \u001B[38;5;66;03m# Create the model\u001B[39;00m\n\u001B[0;32m---> 84\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mSSDModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_classes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# 2 classes: vehicle and non-vehicle\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# Compile the model\u001B[39;00m\n\u001B[1;32m     87\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mssd_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[0;32mIn[24], line 8\u001B[0m, in \u001B[0;36mSSDModel.__init__\u001B[0;34m(self, num_classes)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_classes \u001B[38;5;241m=\u001B[39m num_classes\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Backbone network (e.g. VGG16)\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackbone \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapplications\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mVGG16\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimagenet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minclude_top\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Feature extractor layers\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeature_extractor \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mSequential([\n\u001B[1;32m     12\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mConv2D(\u001B[38;5;241m1024\u001B[39m, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m), activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     13\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mConv2D(\u001B[38;5;241m1024\u001B[39m, (\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m), activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     14\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mMaxPooling2D((\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m     15\u001B[0m ])\n",
      "File \u001B[0;32m~/PycharmProjects/Vehicle-Detection-Project/.venv/lib/python3.12/site-packages/keras/src/applications/vgg16.py:216\u001B[0m, in \u001B[0;36mVGG16\u001B[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation)\u001B[0m\n\u001B[1;32m    209\u001B[0m         weights_path \u001B[38;5;241m=\u001B[39m file_utils\u001B[38;5;241m.\u001B[39mget_file(\n\u001B[1;32m    210\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvgg16_weights_tf_dim_ordering_tf_kernels.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    211\u001B[0m             WEIGHTS_PATH,\n\u001B[1;32m    212\u001B[0m             cache_subdir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    213\u001B[0m             file_hash\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m64373286793e3c8b2b4e3219cbf3544b\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    214\u001B[0m         )\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 216\u001B[0m         weights_path \u001B[38;5;241m=\u001B[39m \u001B[43mfile_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    218\u001B[0m \u001B[43m            \u001B[49m\u001B[43mWEIGHTS_PATH_NO_TOP\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcache_subdir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodels\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfile_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m6d6bbae143d832006294945121d1f1fc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    221\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m     model\u001B[38;5;241m.\u001B[39mload_weights(weights_path)\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m weights \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/Vehicle-Detection-Project/.venv/lib/python3.12/site-packages/keras/src/utils/file_utils.py:295\u001B[0m, in \u001B[0;36mget_file\u001B[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001B[0m\n\u001B[1;32m    293\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(error_msg\u001B[38;5;241m.\u001B[39mformat(origin, e\u001B[38;5;241m.\u001B[39mcode, e\u001B[38;5;241m.\u001B[39mmsg))\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m urllib\u001B[38;5;241m.\u001B[39merror\u001B[38;5;241m.\u001B[39mURLError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 295\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(error_msg\u001B[38;5;241m.\u001B[39mformat(origin, e\u001B[38;5;241m.\u001B[39merrno, e\u001B[38;5;241m.\u001B[39mreason))\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mException\u001B[39;00m, \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m):\n\u001B[1;32m    297\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(fpath):\n",
      "\u001B[0;31mException\u001B[0m: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5: None -- [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "class SSDModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SSDModel, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Backbone network (e.g. VGG16)\n",
    "        self.backbone = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "        # Feature extractor layers\n",
    "        self.feature_extractor = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        ])\n",
    "\n",
    "        # SSD layers\n",
    "        self.ssd_layers = []\n",
    "        for i in range(6):  # 6 default boxes per feature map\n",
    "            self.ssd_layers.append(SSDLayer(4 + i, num_classes))\n",
    "\n",
    "        # Prediction layers\n",
    "        self.prediction_layers = []\n",
    "        for i in range(6):  # 6 feature maps\n",
    "            self.prediction_layers.append(PredictionLayer(num_classes))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Backbone network\n",
    "        x = self.backbone(inputs)\n",
    "\n",
    "        # Feature extractor layers\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        # SSD layers\n",
    "        feature_maps = []\n",
    "        for i, layer in enumerate(self.ssd_layers):\n",
    "            x, feature_map = layer(x)\n",
    "            feature_maps.append(feature_map)\n",
    "\n",
    "        # Prediction layers\n",
    "        predictions = []\n",
    "        for i, layer in enumerate(self.prediction_layers):\n",
    "            predictions.append(layer(feature_maps[i]))\n",
    "\n",
    "        # Concatenate predictions\n",
    "        predictions = tf.concat(predictions, axis=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class SSDLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_defaults, num_classes):\n",
    "        super(SSDLayer, self).__init__()\n",
    "        self.num_defaults = num_defaults\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv = tf.keras.layers.Conv2D(self.num_defaults * (4 + self.num_classes), (3, 3), activation='linear', padding='same', input_shape=input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = tf.reshape(x, (-1, x.shape[1], x.shape[2], self.num_defaults, 4 + self.num_classes))\n",
    "        return x, x[:, :, :, :, :4]\n",
    "\n",
    "class PredictionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PredictionLayer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv = tf.keras.layers.Conv2D(self.num_classes, (3, 3), activation='softmax', padding='same', input_shape=input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        return x\n",
    "\n",
    "def load_image(file_path, label):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_image(img, channels=3)  # Ensure 3 channels for color images\n",
    "    img = tf.cast(img, tf.uint8)  # Cast the image to uint8 data type\n",
    "    return img, label\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model = SSDModel(num_classes=2)  # 2 classes: vehicle and non-vehicle\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='ssd_loss', metrics=['accuracy'])\n",
    "\n",
    "# Create a dataset object\n",
    "dataset = image_dataset.map(load_image)\n",
    "\n",
    "# # Define the size of the training set\n",
    "# train_size = int(0.8 * dataset.cardinality().numpy())\n",
    "\n",
    "\n",
    "# # Split the dataset into training and testing subsets\n",
    "# train_dataset = dataset.take(train_size)\n",
    "# test_dataset = dataset.skip(train_size)\n",
    "\n",
    "# Convert dataset to list to calculate train_size\n",
    "dataset_list = list(dataset)\n",
    "train_size = int(0.8 * len(dataset_list))\n",
    "\n",
    "# Split the dataset into training and testing subsets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(dataset_list[:train_size])\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(dataset_list[train_size:])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
